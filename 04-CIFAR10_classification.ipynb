{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of CIFAR10 Dataset using Convolutional Neural Networks\n",
        "\n",
        "## Introduction\n",
        "The CIFAR10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "We will use a Convolutional Neural Network (CNN) to classify the images in the CIFAR10 dataset."
      ],
      "metadata": {
        "id": "r25-wLFpruMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set Up\n",
        "Install and import the necessary libraries."
      ],
      "metadata": {
        "id": "1re8lnspsqRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gSulmEDprh_8"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries\n",
        "\n",
        "#!pip install torch\n",
        "#!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset"
      ],
      "metadata": {
        "id": "MziFuqBhroB_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.beckends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JSpN9cGrqiC",
        "outputId": "fa537049-3899-41dd-8ff0-3b971384ef8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Data\n",
        "We'll download the CIFAR10 dataset from teh torchvision library.\n",
        "\n",
        "**Note: CIFAR10 dataset input size is 3x32x32**"
      ],
      "metadata": {
        "id": "3Gtnp207tUVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image path if using google drive\n",
        "img_path = '/content/data'"
      ],
      "metadata": {
        "id": "Pe_WpJTArqf6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set image path if using local device\n",
        "#img_path = './data'"
      ],
      "metadata": {
        "id": "1WQCVojRrqeC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare to transform the image to tensors\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Download CIFAR10 train dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=img_path, train=True, download=True, transform=transform)\n",
        "train_dataset = Subset(train_dataset, range(10000)) # Use 10,000 images instead of the total 50,0000\n",
        "\n",
        "# Download CIFAR10 test dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=img_path, train=False, download=True, transform=transform)\n",
        "test_dataset = Subset(test_dataset, range(1000)) # Use 1,000 images instead of the total 10,000\n",
        "\n",
        "# Load training dataset\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=512,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load testing dataset\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=512,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVAA_KC0rqcG",
        "outputId": "1a0f0629-1033-4b28-ecdd-4e3791d809c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Convolutional Neural Network (CNN)\n",
        "\n",
        "We will define a CNN with the following architecture<br>\n",
        "* Convolutional Layer 1\n",
        "* Convolutional Layer 2\n",
        "* Max Pooling 1\n",
        "* Convolutional Layer 3\n",
        "* Convolutional Layer 4\n",
        "* Max Pooling 2\n",
        "* Flatten\n",
        "* Fully Connected Layer 1\n",
        "* Fully Connected Layer 2\n",
        "* Fully Connected Layer 3\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UmYfRKWqvKDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Create CNN Class"
      ],
      "metadata": {
        "id": "yoKRl0smwcyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.Sequential = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1, stride=1), # Input: 3x32x32, Output: 32x32x32\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1), # Input: 32x32x32, Output: 32x32x32\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # Input: 32x32x32, Output: 32x16x16\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1), # Input: 32x16x16, Output: 64x16x16\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1), # Input: 64x16x16, Output: 64x16x16\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # Input: 64x16x16, Output: 64x8x8\n",
        "        nn.Flatten(), # Output: 64*8*8\n",
        "        nn.Linear(in_features=64*8*8, out_features=1024), # Input: 63*8*8, Output: 1024\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(in_features=1024, out_features=512), # Input: 1028, Output: 512\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=10) # Input: 128, Output: 10\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.Sequential(x)"
      ],
      "metadata": {
        "id": "IbWQhk7wvJvn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train the Model"
      ],
      "metadata": {
        "id": "6fwd2HQSyn8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = ConvNet()\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# Set loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set number of epochs\n",
        "epochs = 20\n",
        "\n",
        "# Train model\n",
        "for epoch in range(epochs):\n",
        "  # Track accuracy\n",
        "  accuracy_history = 0\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "    # Forward pass\n",
        "    pred = model(data)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = loss_fn(pred, targets)\n",
        "\n",
        "    # Run backprogation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    _, predicted = torch.max(pred.data, 1)\n",
        "    accuracy = (predicted == targets).sum().item() / targets.size(0)\n",
        "    accuracy_history += accuracy\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}, Loss: {loss.item():.4f}, Accuracy: {accuracy_history/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWMoFt0FrqaZ",
        "outputId": "0fa10925-dae0-4c8b-d208-b3636fda78ae"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 2.2832, Accuracy: 0.1261\n",
            "Epoch: 2, Loss: 2.1517, Accuracy: 0.1830\n",
            "Epoch: 3, Loss: 1.9857, Accuracy: 0.2447\n",
            "Epoch: 4, Loss: 1.8417, Accuracy: 0.3060\n",
            "Epoch: 5, Loss: 1.8888, Accuracy: 0.3248\n",
            "Epoch: 6, Loss: 1.6405, Accuracy: 0.3699\n",
            "Epoch: 7, Loss: 1.4457, Accuracy: 0.4405\n",
            "Epoch: 8, Loss: 1.3591, Accuracy: 0.4790\n",
            "Epoch: 9, Loss: 1.4932, Accuracy: 0.5183\n",
            "Epoch: 10, Loss: 1.1058, Accuracy: 0.5542\n",
            "Epoch: 11, Loss: 1.1055, Accuracy: 0.6132\n",
            "Epoch: 12, Loss: 1.0621, Accuracy: 0.6486\n",
            "Epoch: 13, Loss: 0.8866, Accuracy: 0.7088\n",
            "Epoch: 14, Loss: 0.7068, Accuracy: 0.7547\n",
            "Epoch: 15, Loss: 0.7408, Accuracy: 0.7901\n",
            "Epoch: 16, Loss: 0.4709, Accuracy: 0.8187\n",
            "Epoch: 17, Loss: 0.4980, Accuracy: 0.8491\n",
            "Epoch: 18, Loss: 0.2772, Accuracy: 0.8884\n",
            "Epoch: 19, Loss: 0.4417, Accuracy: 0.8984\n",
            "Epoch: 20, Loss: 0.3357, Accuracy: 0.9002\n"
          ]
        }
      ]
    }
  ]
}